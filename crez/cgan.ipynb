{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def show_images(images):\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "    \n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./datasets/MNIST_data', one_hot=True) # include one-hot labels\n",
    "\n",
    "# show a batch\n",
    "show_images(mnist.train.next_batch(16)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfrecords_filename = './datasets/celeba/celeba_train.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reconstructed_images = []\n",
    "\n",
    "# record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "\n",
    "# for string_record in record_iterator:\n",
    "    \n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(string_record)\n",
    "    \n",
    "#     height = int(example.features.feature['height']\n",
    "#                                  .int64_list\n",
    "#                                  .value[0])\n",
    "    \n",
    "#     width = int(example.features.feature['width']\n",
    "#                                 .int64_list\n",
    "#                                 .value[0])\n",
    "    \n",
    "#     height = (example.features.feature['height']\n",
    "#                                   .int64_list\n",
    "#                                   .value[0])\n",
    "\n",
    "#     img_string = (example.features.feature['image_raw']\n",
    "#                                 .bytes_list\n",
    "#                                 .value[0])\n",
    "    \n",
    "#     img_1d = np.fromstring(img_string, dtype=np.uint8)\n",
    "#     reconstructed_img = img_1d.reshape((height, width, -1))\n",
    "    \n",
    "# #     annotation_1d = np.fromstring(annotation_string, dtype=np.uint8)\n",
    "    \n",
    "#     # Annotations don't have depth (3rd dimension)\n",
    "# #     reconstructed_annotation = annotation_1d.reshape((height, width))\n",
    "    \n",
    "# #     reconstructed_images.append((reconstructed_img, reconstructed_annotation))\n",
    "#     reconstructed_images.append(reconstructed_img)\n",
    "\n",
    "    \n",
    "# # Let's check if reconstructed images match\n",
    "# for reconstructed in reconstructed_images[:5]:\n",
    "#     plt.imshow(reconstructed)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import skimage.io as io\n",
    "\n",
    "# IMAGE_HEIGHT = 384\n",
    "# IMAGE_WIDTH = 384\n",
    "\n",
    "\n",
    "# def read_and_decode(filename_queue):\n",
    "    \n",
    "#     reader = tf.TFRecordReader()\n",
    "\n",
    "#     _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "#     features = tf.parse_single_example(\n",
    "#       serialized_example,\n",
    "#       # Defaults are not specified since both keys are required.\n",
    "#       features={\n",
    "#         'height': tf.FixedLenFeature([], tf.int64),\n",
    "#         'width': tf.FixedLenFeature([], tf.int64),\n",
    "#         'depth': tf.FixedLenFeature([], tf.int64),\n",
    "#         'image_raw': tf.FixedLenFeature([], tf.string)\n",
    "#         })\n",
    "\n",
    "#     # Convert from a scalar string tensor (whose single string has\n",
    "#     # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "#     # [mnist.IMAGE_PIXELS].\n",
    "#     image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "# #     annotation = tf.decode_raw(features['mask_raw'], tf.uint8)\n",
    "    \n",
    "#     height = tf.cast(features['height'], tf.int32)\n",
    "#     width = tf.cast(features['width'], tf.int32)\n",
    "    \n",
    "#     image_shape = tf.stack([height, width, 3])\n",
    "# #     annotation_shape = tf.stack([height, width, 1])\n",
    "    \n",
    "#     image = tf.reshape(image, image_shape)\n",
    "# #     annotation = tf.reshape(annotation, annotation_shape)\n",
    "    \n",
    "#     image_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.int32)\n",
    "# #     annotation_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=tf.int32)\n",
    "    \n",
    "#     # Random transformations can be put here: right before you crop images\n",
    "#     # to predefined size. To get more information look at the stackoverflow\n",
    "#     # question linked above.\n",
    "    \n",
    "#     resized_image = tf.image.resize_image_with_crop_or_pad(image=image,\n",
    "#                                            target_height=IMAGE_HEIGHT,\n",
    "#                                            target_width=IMAGE_WIDTH)\n",
    "    \n",
    "# #     resized_annotation = tf.image.resize_image_with_crop_or_pad(image=annotation,\n",
    "# #                                            target_height=IMAGE_HEIGHT,\n",
    "# #                                            target_width=IMAGE_WIDTH)\n",
    "    \n",
    "    \n",
    "# #     images, annotations = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "# #                                                  batch_size=2,\n",
    "# #                                                  capacity=30,\n",
    "# #                                                  num_threads=2,\n",
    "# #                                                  min_after_dequeue=10)\n",
    "#     images = tf.train.shuffle_batch( [resized_image],\n",
    "#                                                  batch_size=2,\n",
    "#                                                  capacity=30,\n",
    "#                                                  num_threads=2,\n",
    "#                                                  min_after_dequeue=10)\n",
    "#     annotations = None\n",
    "    \n",
    "#     return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename_queue = tf.train.string_input_producer(\n",
    "#     [tfrecords_filename], num_epochs=10)\n",
    "\n",
    "# # Even when reading in multiple threads, share the filename\n",
    "# # queue.\n",
    "# image, annotation = read_and_decode(filename_queue)\n",
    "\n",
    "# # The op for initializing the variables.\n",
    "# init_op = tf.group(tf.global_variables_initializer(),\n",
    "#                    tf.local_variables_initializer())\n",
    "\n",
    "# with tf.Session()  as sess:\n",
    "    \n",
    "#     sess.run(init_op)\n",
    "    \n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "#     # Let's read off 3 batches just for example\n",
    "#     for i in range(3):\n",
    "    \n",
    "#         img = sess.run(image)\n",
    "#         io.imshow(img[0, :, :, :])\n",
    "#         io.show()\n",
    "        \n",
    "    \n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "\n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "\n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    return tf.maximum(x, alpha*x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"Generate random uniform noise from -1 to 1.\n",
    "\n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size of noise to generate\n",
    "    - dim: integer giving the dimension of the the noise to generate\n",
    "\n",
    "    Returns:\n",
    "    TensorFlow Tensor containing uniform noise in [-1, 1] with shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    return tf.random_uniform((batch_size, dim), minval=-1, maxval=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downsampled_img(batch_size, img):\n",
    "    \"\"\"Downsample image\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size\n",
    "    - img: input image\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of downsized img\n",
    "    \"\"\"\n",
    "    # sample and return noise\n",
    "    img = tf.reshape(img, (batch_size, 28, 28, 1))\n",
    "    x_small = tf.image.resize_bicubic(img, (14, 14))\n",
    "    x_small = tf.reshape(x_small, (batch_size, -1))\n",
    "    return x_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solvers(learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"Create solvers for GAN training.\n",
    "\n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "\n",
    "    Returns:\n",
    "    - D_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, y):\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "\n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, 784]\n",
    "\n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score\n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        inputs = tf.concat(values=[x, y], axis=1)\n",
    "\n",
    "        fc1 = tf.layers.dense(inputs=inputs, units=256, activation=leaky_relu)\n",
    "        fc2 = tf.layers.dense(inputs=fc1, units=256, activation=leaky_relu)\n",
    "        logits = tf.layers.dense(inputs=fc2, units=1, activation=None)\n",
    "        return logits\n",
    "\n",
    "def generator(z, y):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "\n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "\n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 784].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        inputs = tf.concat(values=[z, y], axis=1)\n",
    "\n",
    "        fc1 = tf.layers.dense(inputs=inputs, units=1024, activation=tf.nn.relu)\n",
    "        fc2 = tf.layers.dense(inputs=fc1, units=1024, activation=tf.nn.relu)\n",
    "        img = tf.layers.dense(inputs=fc2, units=784, activation=tf.nn.tanh)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gan_loss(logits_real, logits_fake):\n",
    "    \"\"\"Compute the GAN loss.\n",
    "\n",
    "    Inputs:\n",
    "    - logits_real: Tensor, shape [batch_size, 1], output of discriminator\n",
    "        Log probability that the image is real for each real image\n",
    "    - logits_fake: Tensor, shape[batch_size, 1], output of discriminator\n",
    "        Log probability that the image is real for each fake image\n",
    "\n",
    "    Returns:\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \"\"\"\n",
    "    D_real_labels = tf.ones_like(logits_real)\n",
    "    D_real_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=D_real_labels, logits=logits_real)\n",
    "    D_false_labels = tf.zeros_like(logits_fake)\n",
    "    D_fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=D_false_labels, logits=logits_fake)\n",
    "    D_loss = tf.reduce_mean(D_real_loss)+tf.reduce_mean(D_fake_loss)\n",
    "\n",
    "    G_false_labels = tf.ones_like(logits_fake)\n",
    "    G_fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=G_false_labels, logits=logits_fake)\n",
    "    G_loss = tf.reduce_mean(G_fake_loss)\n",
    "\n",
    "    return D_loss, G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_a_gan(sess, G_train_step, G_loss, D_train_step, D_loss, G_extra_step, D_extra_step,              show_every=250, print_every=50, batch_size=128, num_epoch=10):\n",
    "    \"\"\"Train a GAN for a certain number of epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - sess: A tf.Session that we want to use to run our data\n",
    "    - G_train_step: A training step for the Generator\n",
    "    - G_loss: Generator loss\n",
    "    - D_train_step: A training step for the Generator\n",
    "    - D_loss: Discriminator loss\n",
    "    - G_extra_step: A collection of tf.GraphKeys.UPDATE_OPS for generator\n",
    "    - D_extra_step: A collection of tf.GraphKeys.UPDATE_OPS for discriminator\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    # compute the number of iterations we need\n",
    "    max_iter = int(mnist.train.num_examples*num_epoch/batch_size)\n",
    "    for it in range(max_iter):\n",
    "        # every show often, show a sample result\n",
    "        if it % show_every == 0:\n",
    "            num_samples = 16\n",
    "\n",
    "            y_sample = np.zeros(shape=[batch_size, num_labels])\n",
    "            y_sample[:, 5] = 1 # condition on class=5\n",
    "\n",
    "            samples = sess.run(G_sample, feed_dict={y:y_sample})\n",
    "            fig = show_images(samples[:num_samples])\n",
    "            plt.show()\n",
    "            print()\n",
    "        # run a batch of data through the network\n",
    "        minibatch,minbatch_y = mnist.train.next_batch(batch_size)\n",
    "        _, D_loss_curr = sess.run([D_train_step, D_loss], feed_dict={x: minibatch, y:minbatch_y})\n",
    "        _, G_loss_curr = sess.run([G_train_step, G_loss], feed_dict={x: minibatch, y:minbatch_y})\n",
    "\n",
    "        # print loss every so often.\n",
    "        # We want to make sure D_loss doesn't go to 0\n",
    "        if it % print_every == 0:\n",
    "            print('Iter: {}, D: {:.4}, G:{:.4}'.format(it,D_loss_curr,G_loss_curr))\n",
    "    print('Final images')\n",
    "\n",
    "    y_sample = np.zeros(shape=[batch_size, num_labels])\n",
    "    y_sample[:, 5] = 1 # condition on class=5\n",
    "    samples = sess.run(G_sample, feed_dict={y:y_sample})\n",
    "\n",
    "    fig = show_images(samples[:16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# number of images for each batch\n",
    "batch_size = 128\n",
    "\n",
    "# number of labels\n",
    "num_labels = mnist.train.labels.shape[1]\n",
    "\n",
    "# our noise dimension\n",
    "noise_dim = 96\n",
    "\n",
    "\n",
    "# placeholder for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "\n",
    "# random noise fed into our generator\n",
    "z = sample_noise(batch_size, noise_dim)\n",
    "# generated images\n",
    "G_sample = generator(z, y)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    #scale images to be -1 to 1\n",
    "    logits_real = discriminator(preprocess_img(x), y)\n",
    "    # Re-use discriminator weights on new inputs\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample, y)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')\n",
    "\n",
    "# get our solver\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# get our loss\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "\n",
    "# setup training steps\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "D_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'discriminator')\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,G_train_step,G_loss,D_train_step,D_loss,G_extra_step,D_extra_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
